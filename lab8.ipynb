{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Varshini-svnit/ML_LABS/blob/main/lab8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eecebac-7197-4148-9dc3-a221aab9a001",
      "metadata": {
        "id": "3eecebac-7197-4148-9dc3-a221aab9a001"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import KFold\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ce5e78a-7f93-43fb-8eb8-bc7919750fdc",
      "metadata": {
        "id": "4ce5e78a-7f93-43fb-8eb8-bc7919750fdc"
      },
      "outputs": [],
      "source": [
        "def accuracy_from_scratch(y_true, y_pred):\n",
        "    return np.sum(y_true == y_pred) / len(y_true)\n",
        "\n",
        "def precision_recall_f1_from_scratch(y_true, y_pred, average='macro'):\n",
        "    classes = np.unique(np.concatenate((y_true, y_pred)))\n",
        "    all_precisions, all_recalls, all_f1s = [], [], []\n",
        "    for cls in classes:\n",
        "        tp = np.sum((y_true == cls) & (y_pred == cls))\n",
        "        fp = np.sum((y_true != cls) & (y_pred == cls))\n",
        "        fn = np.sum((y_true == cls) & (y_pred != cls))\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        all_precisions.append(precision)\n",
        "        all_recalls.append(recall)\n",
        "        all_f1s.append(f1)\n",
        "    return np.mean(all_precisions), np.mean(all_recalls), np.mean(all_f1s)\n",
        "\n",
        "def evaluate_classifier_from_scratch(y_true, y_pred):\n",
        "    accuracy = accuracy_from_scratch(y_true, y_pred)\n",
        "    precision, recall, f1 = precision_recall_f1_from_scratch(y_true, y_pred)\n",
        "    return accuracy, precision, recall, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b985350-e90c-487f-b4a3-9da1a19a6b65",
      "metadata": {
        "id": "9b985350-e90c-487f-b4a3-9da1a19a6b65"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, n_iters=1000, penalty='none', C=1.0, l1_ratio=0.5):\n",
        "        self.lr = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "        self.penalty = penalty\n",
        "        self.C = C  # Inverse of regularization strength\n",
        "        self.l1_ratio = l1_ratio\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        z = np.array(z, dtype=float)\n",
        "        z = np.clip(z, -500, 500)\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_predicted = self._sigmoid(linear_model)\n",
        "\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            if self.penalty == 'l2':\n",
        "                dw += (1 / self.C) * self.weights\n",
        "            elif self.penalty == 'l1':\n",
        "                dw += (1 / self.C) * np.sign(self.weights)\n",
        "            elif self.penalty == 'elastic_net':\n",
        "                dw += (1 / self.C) * (self.l1_ratio * np.sign(self.weights) + (1 - self.l1_ratio) * self.weights)\n",
        "\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        return self._sigmoid(linear_model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([1 if i > 0.5 else 0 for i in self.predict_proba(X)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "026a19e3-240a-4584-9dc1-63587e45b042",
      "metadata": {
        "id": "026a19e3-240a-4584-9dc1-63587e45b042"
      },
      "outputs": [],
      "source": [
        "class OneVsRestClassifier:\n",
        "    def __init__(self, base_classifier):\n",
        "        self.base_classifier_prototype = base_classifier\n",
        "        self.classifiers, self.classes = [], []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = X.astype(float)\n",
        "        y = y.astype(float)\n",
        "        self.classes = np.unique(y)\n",
        "        self.classifiers = []\n",
        "        for cls in self.classes:\n",
        "            y_binary = np.where(y == cls, 1, 0)\n",
        "            classifier = copy.deepcopy(self.base_classifier_prototype)\n",
        "            classifier.fit(X, y_binary)\n",
        "            self.classifiers.append(classifier)\n",
        "\n",
        "    def predict(self, X):\n",
        "        probabilities = np.array([clf.predict_proba(X) for clf in self.classifiers]).T\n",
        "        return np.array([self.classes[np.argmax(prob)] for prob in probabilities])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "943b70fd-7f93-4934-b0c5-eb8a8aefdbdf",
      "metadata": {
        "id": "943b70fd-7f93-4934-b0c5-eb8a8aefdbdf"
      },
      "outputs": [],
      "source": [
        "class KNeighborsClassifier:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def _euclidean_distance(self, x1, x2):\n",
        "        return np.sqrt(np.sum((x1 - x2)**2))\n",
        "\n",
        "    def _predict_single(self, x):\n",
        "        distances = [self._euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "        return Counter(k_nearest_labels).most_common(1)[0][0]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict_single(x) for x in X])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "377d9870-a4f7-4500-9583-cf382dd5d9e5",
      "metadata": {
        "id": "377d9870-a4f7-4500-9583-cf382dd5d9e5"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('drug_200.csv')\n",
        "df = pd.get_dummies(df, columns=['Sex', 'BP', 'Cholesterol'], drop_first=True)\n",
        "drug_mapping = {drug: i for i, drug in enumerate(df['Drug'].unique())}\n",
        "df['Drug'] = df['Drug'].map(drug_mapping)\n",
        "\n",
        "for col in ['Age', 'Na_to_K']:\n",
        "    mean, std = df[col].mean(), df[col].std()\n",
        "    df[col] = (df[col] - mean) / std\n",
        "\n",
        "X = df.drop('Drug', axis=1).values\n",
        "y = df['Drug'].values\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a734be26-23c8-412f-95ce-9ec1f67f2d3a",
      "metadata": {
        "id": "a734be26-23c8-412f-95ce-9ec1f67f2d3a",
        "outputId": "743d3b8b-a737-4d68-8c50-015783d563f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Task 1: Logistic Regression ---\n",
            "\n",
            "Results for No Regularization:\n",
            "  Average Accuracy: 0.9350\n",
            "  Average Precision: 0.9500\n",
            "  Average Recall: 0.9159\n",
            "  Average F1-score: 0.9187\n",
            "\n",
            "Results for Lasso (L1):\n",
            "  Average Accuracy: 0.4550\n",
            "  Average Precision: 0.0965\n",
            "  Average Recall: 0.2100\n",
            "  Average F1-score: 0.1318\n",
            "\n",
            "Results for Ridge (L2):\n",
            "  Average Accuracy: 0.4550\n",
            "  Average Precision: 0.0965\n",
            "  Average Recall: 0.2100\n",
            "  Average F1-score: 0.1318\n",
            "\n",
            "Results for Elastic Net:\n",
            "  Average Accuracy: 0.4550\n",
            "  Average Precision: 0.0965\n",
            "  Average Recall: 0.2100\n",
            "  Average F1-score: 0.1318\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Task 1: Logistic Regression ---\")\n",
        "penalties = {'No Regularization': 'none', 'Lasso (L1)': 'l1', 'Ridge (L2)': 'l2', 'Elastic Net': 'elastic_net'}\n",
        "for name, penalty in penalties.items():\n",
        "    scores = []\n",
        "    lr_prototype = LogisticRegression(learning_rate=0.1, n_iters=1000, penalty=penalty, C=1.0)\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "        # --- FIX: Instantiate OvR inside the loop for a clean state ---\n",
        "        ovr = OneVsRestClassifier(base_classifier=lr_prototype)\n",
        "        ovr.fit(X_train, y_train)\n",
        "        predictions = ovr.predict(X_test)\n",
        "        scores.append(evaluate_classifier_from_scratch(y_test, predictions))\n",
        "    avg_scores = np.mean(scores, axis=0)\n",
        "    print(f\"\\nResults for {name}:\\n  Average Accuracy: {avg_scores[0]:.4f}\\n  Average Precision: {avg_scores[1]:.4f}\\n  Average Recall: {avg_scores[2]:.4f}\\n  Average F1-score: {avg_scores[3]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5dfa40d-fcea-4b2a-bd79-0cfb10b51525",
      "metadata": {
        "id": "a5dfa40d-fcea-4b2a-bd79-0cfb10b51525",
        "outputId": "2941561e-2045-4533-d401-fd2f2102ad89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "--- Task 2: K-Nearest Neighbors ---\n",
            "\n",
            "Results for K=1:\n",
            "  Average Accuracy: 0.8900\n",
            "  Average Precision: 0.8637\n",
            "  Average Recall: 0.9239\n",
            "  Average F1-score: 0.8755\n",
            "\n",
            "Results for K=3:\n",
            "  Average Accuracy: 0.8600\n",
            "  Average Precision: 0.7853\n",
            "  Average Recall: 0.8511\n",
            "  Average F1-score: 0.8007\n",
            "\n",
            "Results for K=5:\n",
            "  Average Accuracy: 0.8900\n",
            "  Average Precision: 0.8382\n",
            "  Average Recall: 0.8516\n",
            "  Average F1-score: 0.8316\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\\n--- Task 2: K-Nearest Neighbors ---\")\n",
        "k_values = [1, 3, 5]\n",
        "for k in k_values:\n",
        "    scores = []\n",
        "    knn = KNeighborsClassifier(k=k)\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        knn.fit(X_train, y_train)\n",
        "        predictions = knn.predict(X_test)\n",
        "        scores.append(evaluate_classifier_from_scratch(y_test, predictions))\n",
        "\n",
        "    avg_scores = np.mean(scores, axis=0)\n",
        "    print(f\"\\nResults for K={k}:\")\n",
        "    print(f\"  Average Accuracy: {avg_scores[0]:.4f}\")\n",
        "    print(f\"  Average Precision: {avg_scores[1]:.4f}\")\n",
        "    print(f\"  Average Recall: {avg_scores[2]:.4f}\")\n",
        "    print(f\"  Average F1-score: {avg_scores[3]:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}